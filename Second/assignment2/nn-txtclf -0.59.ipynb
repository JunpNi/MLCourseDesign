{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "import matplotlib\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a dataset\n",
    "\n",
    "Let's start by generating a dataset we can play with. Fortunately, [scikit-learn](http://scikit-learn.org/) has some useful dataset generators, so we don't need to write the code ourselves. We will go with the [`make_moons`](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: rych@festival.ed.ac.uk (R Hawkes)\n",
      "Subject: 3DS: Where did all the texture rules go?\n",
      "Lines: 21\n",
      "\n",
      "Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n",
      "\n",
      "======================================================================\n",
      "Rycharde Hawkes\t\t\t\temail: rych@festival.ed.ac.uk\n",
      "Virtual Environment Laboratory\n",
      "Dept. of Psychology\t\t\tTel  : +44 31 650 3426\n",
      "Univ. of Edinburgh\t\t\tFax  : +44 31 667 0150\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate a dataset and plot it\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',  categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',  categories=categories)\n",
    "\n",
    "print(newsgroups_train.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 80) (2034,)\n",
      "(1353, 80) (1353,)\n"
     ]
    }
   ],
   "source": [
    "num_train = len(newsgroups_train.data)\n",
    "num_test  = len(newsgroups_test.data)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=80)\n",
    "\n",
    "X = vectorizer.fit_transform( newsgroups_train.data + newsgroups_test.data )\n",
    "X_train = X[0:num_train, :]\n",
    "X_test = X[num_train:num_train+num_test,:]\n",
    "\n",
    "Y_train = newsgroups_train.target\n",
    "Y_test = newsgroups_test.target\n",
    "\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to plot a decision boundary.\n",
    "# If you don't fully understand this function don't worry, it just generates the contour plot below.\n",
    "def plot_decision_boundary(pred_func):\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to evaluate the total loss on the dataset\n",
    "def calculate_loss(model,X, y):\n",
    "    W1, b1, W2, b2, W3, b3, W4, b4, W5, b5 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'], model['b3'], model['W4'], model['b4'], model['W5'], model['b5']\n",
    "    # Forward propagation to calculate our predictions\n",
    "    z1 = X.dot(W1) + b1\n",
    "    a1 = (abs(z1) + z1) / 2\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    a2 = (abs(z2) + z2) / 2\n",
    "    z3 = a2.dot(W3) + b3\n",
    "    a3 = (abs(z3) + z3) / 2\n",
    "    z4 = a3.dot(W4) + b4\n",
    "    a4 = (abs(z4) + z4) / 2\n",
    "    z5 = a4.dot(W5) + b5\n",
    "    #exp_scores = np.exp(z2)\n",
    "    exp_scores = np.exp(z5)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    # Calculating the loss\n",
    "    corect_logprobs = -np.log(probs[range(num_examples), y])\n",
    "    data_loss = np.sum(corect_logprobs)\n",
    "    # Add regulatization term to loss (optional)\n",
    "    data_loss += reg_lambda/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3)) + np.sum(np.square(W4)) + np.sum(np.square(W5)))\n",
    "    return 1./num_examples * data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also implement a helper function to calculate the output of the network. It does forward propagation as defined above and returns the class with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to predict an output (0 or 1)\n",
    "def predict(model, x):\n",
    "    W1, b1, W2, b2, W3, b3, W4, b4, W5, b5 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'], model['b3'], model['W4'], model['b4'], model['W5'], model['b5']\n",
    "    # Forward propagation\n",
    "    z1 = x.dot(W1) + b1\n",
    "    a1 = (abs(z1) + z1) / 2\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    a2 = (abs(z2) + z2) / 2\n",
    "    z3 = a2.dot(W3) + b3\n",
    "    a3 = (abs(z3) + z3) / 2\n",
    "    z4 = a3.dot(W4) + b4\n",
    "    a4 = (abs(z4) + z4) / 2\n",
    "    z5 = a4.dot(W5) + b5\n",
    "    #exp_scores = np.exp(z2)\n",
    "    exp_scores = np.exp(z5)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    return np.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, here comes the function to train our Neural Network. It implements batch gradient descent using the backpropagation derivates we found above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function learns parameters for the neural network and returns the model.\n",
    "# - nn_hdim: Number of nodes in the hidden layer\n",
    "# - num_passes: Number of passes through the training data for gradient descent\n",
    "# - print_loss: If True, print the loss every 1000 iterations\n",
    "def build_model(X, y, nn_hdim, epsilon=0.01, reg_lambda=0, num_passes=20000,  print_loss=False):\n",
    "    \n",
    "    # Initialize the parameters to random values. We need to learn these.\n",
    "    np.random.seed(0)\n",
    "    nn_input_dim=80\n",
    "    nn_output_dim=4\n",
    "    W1 = np.random.randn(nn_input_dim, nn_hdim[0]) / np.sqrt(nn_input_dim)\n",
    "    b1 = np.zeros((1, nn_hdim[0]))\n",
    "    W2 = np.random.randn(nn_hdim[0], nn_hdim[1]) / np.sqrt(nn_hdim[0])\n",
    "    b2 = np.zeros((1, nn_hdim[1]))\n",
    "    W3 = np.random.randn(nn_hdim[1], nn_hdim[2]) / np.sqrt(nn_hdim[1])\n",
    "    b3 = np.zeros((1, nn_hdim[2]))\n",
    "    W4 = np.random.randn(nn_hdim[2], nn_hdim[3]) / np.sqrt(nn_hdim[2])\n",
    "    b4 = np.zeros((1, nn_hdim[3]))\n",
    "    W5 = np.random.randn(nn_hdim[3], nn_output_dim) / np.sqrt(nn_hdim[3])\n",
    "    b5 = np.zeros((1, nn_output_dim))\n",
    "    # This is what we return at the end\n",
    "    model = {}\n",
    "    \n",
    "    # Gradient descent. For each batch...\n",
    "    for i in range(0, num_passes):\n",
    "\n",
    "        # Forward propagation\n",
    "        z1 = X.dot(W1) + b1\n",
    "        a1 = (abs(z1) + z1) / 2\n",
    "        z2 = a1.dot(W2) + b2\n",
    "        a2 = (abs(z2) + z2) / 2\n",
    "        z3 = a2.dot(W3) + b3\n",
    "        a3 = (abs(z3) + z3) / 2\n",
    "        z4 = a3.dot(W4) + b4\n",
    "        a4 = (abs(z4) + z4) / 2\n",
    "        z5 = a4.dot(W5) + b5\n",
    "        #exp_scores = np.exp(z2)\n",
    "        exp_scores = np.exp(z5)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        \n",
    "        #print(z5)\n",
    "        # Backpropagation\n",
    "        delta5 = probs\n",
    "        #print(probs)\n",
    "        delta5[range(num_examples), y] -= 1\n",
    "        #print(probs)\n",
    "        dW5 = (a4.T).dot(delta5)\n",
    "        #print(dW5)\n",
    "        db5 = np.sum(delta5, axis=0, keepdims=True)\n",
    "        #print(db5)\n",
    "        #delta2 = delta3.dot(W3.T) * (1 - np.power(a2, 2))\n",
    "        \n",
    "        tempRelu = a4.copy()\n",
    "        tempRelu[tempRelu > 0] = 1\n",
    "        delta4 = delta5.dot(W5.T) * tempRelu\n",
    "        dW4 = (a3.T).dot(delta4)\n",
    "        db4 = np.sum(delta4, axis=0, keepdims=True)\n",
    "        \n",
    "        tempRelu = a3.copy()\n",
    "        tempRelu[tempRelu > 0] = 1\n",
    "        delta3 = delta4.dot(W4.T) * tempRelu\n",
    "        dW3 = (a2.T).dot(delta3)\n",
    "        db3 = np.sum(delta3, axis=0, keepdims=True)\n",
    "        \n",
    "        tempRelu = a2.copy()\n",
    "        tempRelu[tempRelu > 0] = 1\n",
    "        delta2 = delta3.dot(W3.T) * tempRelu\n",
    "        dW2 = (a1.T).dot(delta2)\n",
    "        db2 = np.sum(delta2, axis=0, keepdims=True)\n",
    "        \n",
    "        tempRelu = a1.copy()\n",
    "        tempRelu[tempRelu > 0] = 1\n",
    "        #print(tempRelu)\n",
    "        delta = delta2.dot(W2.T) * tempRelu\n",
    "        #delta = delta2.dot(W2.T) * (1 - np.power(a1, 2))\n",
    "        dW1 = (X.T).dot(delta)\n",
    "        db1 = np.sum(delta, axis=0)\n",
    "        \n",
    "        # Add regularization terms (b1 and b2 don't have regularization terms)\n",
    "        dW5 += reg_lambda * W5\n",
    "        dW4 += reg_lambda * W4\n",
    "        dW3 += reg_lambda * W3\n",
    "        dW2 += reg_lambda * W2\n",
    "        dW1 += reg_lambda * W1\n",
    "        \n",
    "        # Gradient descent parameter update\n",
    "        W1 += -epsilon * dW1\n",
    "        b1 += -epsilon * db1\n",
    "        W2 += -epsilon * dW2\n",
    "        b2 += -epsilon * db2\n",
    "        W3 += -epsilon * dW3\n",
    "        b3 += -epsilon * db3\n",
    "        W4 += -epsilon * dW4\n",
    "        b4 += -epsilon * db4\n",
    "        W5 += -epsilon * dW5\n",
    "        b5 += -epsilon * db5\n",
    "        # Assign new parameters to the model\n",
    "        model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3': W3, 'b3': b3, 'W4': W4, 'b4': b4, 'W5': W5, 'b5': b5}\n",
    "        \n",
    "        # Optionally print the loss.\n",
    "        # This is expensive because it uses the whole dataset, so we don't want to do it too often.\n",
    "        if print_loss and i % 1000 == 0:\n",
    "            print(\"Loss after iteration %i: %f\" %(i, calculate_loss(model,X,y)))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A network with a hidden layer of size 3\n",
    "\n",
    "Let's see what happens if we train a network with a hidden layer size of 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 1.383305\n",
      "Loss after iteration 1000: 0.667686\n",
      "Loss after iteration 2000: 0.434912\n",
      "Loss after iteration 3000: 0.221577\n",
      "Loss after iteration 4000: 0.059148\n"
     ]
    }
   ],
   "source": [
    "# Build a model with a 3-dimensional hidden layer\n",
    "num_examples, input_dim = X_train.shape\n",
    "epsilon = 0.0001\n",
    "reg_lambda = 0.01\n",
    "epochs = 5000\n",
    "\n",
    "model = build_model(X_train, Y_train, [32,32,32,32], epsilon, reg_lambda, epochs, print_loss=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! This looks pretty good. Our neural networks was able to find a decision boundary that successfully separates the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Varying the hidden layer size\n",
    "\n",
    "In the example above we picked a hidden layer size of 3. Let's now get a sense of how varying the hidden layer size affects the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.598670 = 810 / 1353\n"
     ]
    }
   ],
   "source": [
    "n_correct = 0\n",
    "n_test = X_test.shape[0]\n",
    "for n in range(n_test):\n",
    "    x = X_test[n,:]\n",
    "    yp = predict(model, x)\n",
    "    if yp == Y_test[n]:\n",
    "        n_correct += 1.0\n",
    "\n",
    "print('Accuracy %f = %d / %d'%(n_correct/n_test, int(n_correct), n_test) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that while a hidden layer of low dimensionality nicely capture the general trend of our data, but higher dimensionalities are prone to overfitting. They are \"memorizing\" the data as opposed to fitting the general shape. If we were to evaluate our model on a separate test set (and you should!) the model with a smaller hidden layer size would likely perform better because it generalizes better. We could counteract overfitting with stronger regularization, but picking the correct size for hidden layer is a much more \"economical\" solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "Here are some things you can try to become more familiar with the code:\n",
    "\n",
    "1. Instead of batch gradient descent, use minibatch gradient descent ([more info](http://cs231n.github.io/optimization-1/#gd)) to train the network. Minibatch gradient descent typically performs better in practice. \n",
    "2. We used a fixed learning rate $\\epsilon$ for gradient descent. Implement an annealing schedule for the gradient descent learning rate ([more info](http://cs231n.github.io/neural-networks-3/#anneal)). \n",
    "3. We used a $\\tanh$ activation function for our hidden layer. Experiment with other activation functions (some are mentioned above). Note that changing the activation function also means changing the backpropagation derivative.\n",
    "4. Extend the network from two to three classes. You will need to generate an appropriate dataset for this.\n",
    "5. Extend the network to four layers. Experiment with the layer size. Adding another hidden layer means you will need to adjust both the forward propagation as well as the backpropagation code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
